
## ################################ ##
## Configure the linkchecker output ##
## ################################ ##

[output]
status=1   ; print status output
log=text   ; set the logging type
verbose=0  ; switch verbosity on or off
warnings=1 ; report warnings
quiet=0    ; set quiet mode


## ##################### ##
## Configure the Loggers ##
## ##################### ##

[text]

[gml]

[dot]

[csv]

[sql]

[html]

[blacklist]

[xml]

[gxml]

[sitemap]
priority=0.7
#frequency=weekly


## ################################# ##
## Configure Link Checker Operations ##
## ################################# ##

[checking]
threads=10        ; the maximum number of threads
timeout=10        ; connection timeout in seconds
aborttimeout=300  ; grace period awaiting checks to finish after Crl-C
recursionlevel=7  ; the recursion depth to dig into
sslverify=1       ; verify the SSL certificates for https links
maxrunseconds=600 ; Stop checking new urls after these many seconds
maxnumurls=10000  ; maximum number of urls to check

maxrequestspersecond=30   ; max number of requests per second
allowedschemes=http,https ; allowed URL schemes

## Aside from robots.txt which is always checked
## with original LinkChecker user-agent.
useragent=Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)



##################### filtering options ##########################
[filtering]
#ignore=
# ignore everything with 'lconline' in the URL name
#  lconline
# and ignore everything with 'bookmark' in the URL name
#  bookmark
# and ignore all mailto: URLs
#  ^mailto:
# do not recurse into the following URLs

#nofollow=
# just an example
#  http://www\.example\.com/bla

# Ignore specified warnings (see linkchecker -h for the list of
# recognized warnings). Add a comma-separated list of warnings here
# that prevent a valid URL from being logged. Note that the warning
# will be logged in invalid URLs.
#ignorewarnings=url-unicode-domain

# Regular expression to add more URLs recognized as internal links.
# Default is that URLs given on the command line are internal.
#internlinks=^http://www\.example\.net/

# Check external links
checkextern=1


##################### password authentication ##########################
[authentication]


############################ Plugins ###################################

# Check HTML anchors
[AnchorCheck]

# Print HTTP header info
#[HttpHeaderInfo]
# Comma separated list of header prefixes to print.
# The names are case insensitive.
# The default list is empty, so it should be non-empty when activating
# this plugin.
#prefixes=Server,X-

# Add country info to URLs
#[LocationInfo]

# Run W3C syntax checks
#[CssSyntaxCheck]
#[HtmlSyntaxCheck]

# Search for regular expression in page contents
#[RegexCheck]
#warningregex=Oracle Error

# Search for viruses in page contents
#[VirusCheck]
#clamavconf=/etc/clamav/clam.conf

# Check that SSL certificates are at least the given number of days valid.
#[SslCertificateCheck]
#sslcertwarndays=14

# Parse and check links in PDF files
#[PdfParser]

# Parse and check links in Word files
#[WordParser]

# Parse and check links in Markdown files.
# Supported links are:
#        <http://autolink.com>
#        [name](http://link.com "Optional title")
#        [id]: http://link.com "Optional title"
#[MarkdownCheck]
# Regexp of filename
#filename_re=.*\.(blog|markdown|md(own)?|mkdn?)$
